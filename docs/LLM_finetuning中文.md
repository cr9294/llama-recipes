## LLM 微调

在这里，我们讨论使用不同配方对 Llama 2 进行微调。我们将涵盖两种情况：

## 1. **参数高效模型微调**
这有助于使即使在 1 个消费级 GPU 上，微调过程也更加经济实惠。这些方法使我们能够保持整个模型冻结，并只向模型添加微小的可学习参数/层。通过这种方式，我们只训练了一小部分参数。这个类别中最著名的方法是 [LORA](https://arxiv.org/pdf/2106.09685.pdf)、LLaMA Adapter 和 Prefix-tuning。

这些方法将解决三个方面的问题：

- **完整微调的成本** - 这些方法只训练一小组额外的参数，而不是整个模型，这使得在消费级 GPU 上运行成为可能。

- **部署成本** - 对于每个微调的下游模型，我们需要部署一个单独的模型；然而，使用这些方法时，只需使用预训练模型的一小组参数（而不是几个 GB 的参数）即可完成工作。在这种情况下，对于每个任务，我们只在预训练模型的基础上添加这些额外的参数，因此可以将预训练模型视为主干，而这些参数视为不同任务的模型头。

- **灾难性遗忘** - 这些方法还有助于防止在微调中发生的遗忘第一个任务的情况。

HF [PEFT](https://github.com/huggingface/peft) 库提供了在这里使用这些方法的简便方式。请在[这里](https://huggingface.co/blog/peft)了解更多。

## 2. **完整/部分参数微调**

完整参数微调有其优势，在这种方法中，有多种策略可帮助：

- 保持预训练模型冻结，仅微调任务头，例如分类器模型。

- 保持预训练模型冻结，并在顶部添加几个全连接层。

- 对所有层进行微调。

您还可以保持大多数层冻结，并仅微调少数层。有许多不同的技术可供选择，以根据不同的标准冻结/解冻层。


<div style="display: flex;">
    <img src="./images/feature-based_FN.png" alt="Image 1" width="250" />
    <img src="./images/feature-based_FN_2.png" alt="Image 2" width="250" />
    <img src="./images/full-param-FN.png" alt="Image 3" width="250" />
</div>


在这种情况下，取决于模型的大小，您可能需要超越一个 GPU，特别是如果您的模型不能在一个 GPU 中进行训练。在这种情况下，Llama 2 的 7B 参数将不适用于一个 GPU。
您想考虑的方式是，您需要足够的 GPU 内存来保存模型参数、梯度和优化器状态。每个这些，取决于您正在训练的精度，都可以占用多次参数计数 x 精度（取决于是否是 fp32/4 字节、fp16/2 字节或 bf16/2 字节）。
例如，AdamW 优化器为每个参数保留 2 个参数，在许多情况下，这些参数保留在 fp32 中。这意味着根据您正在训练/解冻的层数，您的 GPU 内存可能会超过一个 GPU。

**FSDP（Fully Sharded Data Parallel）**

Pytorch 有用于训练不适应一个 GPU 的模型的 FSDP 包。FSDP 允许您在相同的资源量下训练一个更大的模型。在 FSDP 之前是 DDP（分布式数据并行），其中每个 GPU 持有模型的完整副本，并且只会对数据进行分片。在反向传播的结束时，它会同步梯度。

FSDP 扩展了这个想法，不仅分片数据，还有模型参数、梯度和优化器状态。这意味着每个 GPU 只会保留模型的一个分片。这将导致巨大的内存节省，使我们能够将一个更大的模型放入相同数量的 GPU 中。例如，在 DDP 中，您可以将一个 16GB 内存的 GPU 中最大的模型放入一个大约 700M 参数的模型。所以，假设您有 4 个 GPU，在这种情况下，即使您访问了 4 个 GPU，您仍然无法超越可以放入一个 GPU 中的模型大小。然而，使用 FSDP，您可以将一个 3B 模型放入 4 个 GPU 中，大约是 4 倍大。

请在[此处](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)阅读有关 FSDP 的更多信息，并在[此处](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)开始使用 FSDP。

为了提高使用 FSDP 进行微调的性能，我们可以利用许多功能，例如：

- **混合精度**，在 FSDP 中比 Autocast 更加灵活。它允许用户设置模型参数、缓冲区和梯度的精度。

- **激活检查点**，这是一种通过在前向传播中丢弃中间激活而节省内存的技术，而不是将其保

留在内存中，其成本是在后向传播中重新计算它们。FSDP 激活检查点是分片感知的，这意味着我们需要在用 FSDP 包装模型后应用它。在我们的脚本中，我们正在利用这一点。

- **auto_wrap_policy**，这是指定 FSDP 将如何分区模型的方式，有默认支持变压器包装策略。这使得 FSDP 能够基于模型中的变压器类形成每个 FSDP 单元（模型的分区）。为了识别模型中的这一层，您需要查看包装注意力层和 MLP 的层。这有助于 FSDP 具有更精细的通信单元，有助于优化通信成本。
